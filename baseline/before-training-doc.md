

## For comparision

### GPT-1

- Layers = 12
- Training Token Count = ~4.5 GB of raw text from ~7,000 unpublished books (~985 M words)
- Param Count = 117 million (0.117 B)
- Vocab = 40k iterations of BPE

### GPT-2

- Layers =
- Training Token Count = 40 GB of text, ~15B+ tokens
- Param Count = 1.
- Vocab = ~50k token vocab

### Smoke test model on shakesphere corpus

- Tokenizer vocab loaded from tiny_shakespeare_bpe_vocab.txt
- Vocabulary size: 10000
- Encoded 231,951 tokens
- Model parameters: 29,164,304